@misc{opensauce_2019_2638411,
  author = {Terri M.~Yu and
            R.~David Murray and
            Kate Silverstein and
            Kristine M.~Yu},
  title  = {OpenSauce: Open source software for voice analysis, v0.1},
  year   = 2019,
  doi    = {10.5281/zenodo.2638411},
  url    = {https://github.com/voicesauce/opensauce-python}
}

@article{Moore:1982ha, 
year = {1982}, 
title = {{Suggested formulae for calculating auditory-filter bandwidths and excitation patterns}}, 
author = {Moore, Brian C J and Glasberg, Brian R}, 
journal = {The Journal of the Acoustical Society of America}, 
doi = {10.1121/1.389861}, 
abstract = {{Recent estimates of auditory‐filter shape are used to derive a simple formula relating the equivalent rectangular bandwidth (ERB) of the auditory filter to center frequency. The value of the auditory‐filter bandwidth continues to decrease as center frequency decreases below 500 Hz. A formula is also given relating ERB‐rate to frequency. Finally, a method is described for calculating excitation patterns from filter shapes.}}, 
pages = {750 -- 753}, 
number = {3}, 
volume = {74}, 
language = {English}, 
keywords = {}, 
month = {12}
}

@article{Hawks.1995.10.1121/1.412986, 
year = {1995}, 
title = {{A formant bandwidth estimation procedure for vowel synthesis}}, 
author = {Hawks, John W and Miller, James D}, 
journal = {The Journal of the Acoustical Society of America}, 
issn = {0001-4966}, 
doi = {10.1121/1.412986}, 
pmid = {7876453}, 
abstract = {{The specification of vowel formant bandwidths for speech synthesis has been inconsistent in the past, perhaps due to the difficulty of measuring formant bandwidths in natural speech and the possible perceptual insignificance of formant bandwidths on the intelligibility of synthetic speech. Here, regression equations are presented for the estimation of formant bandwidths based on measurements from natural speech which is based only on formant center frequency and independent of other formant values. Current usage, as well as comparison with another well-known estimation algorithm suggests that the new procedure should be quite acceptable for some types of speech synthesis.}}, 
pages = {1343--1344}, 
number = {2}, 
volume = {97}
}

@article{Fraile.2014.10.1016/j.bspc.2014.07.001, 
year = {2014}, 
title = {{Cepstral peak prominence: A comprehensive analysis}}, 
author = {Fraile, Rubén and Godino-Llorente, Juan Ignacio}, 
journal = {Biomedical Signal Processing and Control}, 
issn = {1746-8094}, 
doi = {10.1016/j.bspc.2014.07.001}, 
abstract = {{An analytical study of cepstral peak prominence (CPP) is presented, intended to provide an insight into its meaning and relation with voice perturbation parameters. To carry out this analysis, a parametric approach is adopted in which voice production is modelled using the traditional source-filter model and the first cepstral peak is assumed to have Gaussian shape. It is concluded that the meaning of CPP is very similar to that of the first rahmonic and some insights are provided on its dependence with fundamental frequency and vocal tract resonances. It is further shown that CPP integrates measures of voice waveform and periodicity perturbations, be them either amplitude, frequency or noise.}}, 
pages = {42--54}, 
volume = {14}, 
keywords = {}
}


@article{Hillenbrand.1994.10.1044/jshr.3704.769, 
year = {1994}, 
title = {{Acoustic Correlates of Breathy Vocal Quality}}, 
author = {Hillenbrand, James and Cleveland, Ronald A. and Erickson, Robert L.}, 
journal = {Journal of Speech, Language, and Hearing Research}, 
issn = {1092-4388}, 
doi = {10.1044/jshr.3704.769}, 
pmid = {7967562}, 
abstract = {{The purpose of this study was to evaluate the effectiveness of several acoustic measures in predicting breathiness ratings. Recordings were made of eight normal men and seven normal women producing normally phonated, moderately breathy, and very breathy sustained vowels. Twenty listeners rated the degree of breathiness using a direct magnitude estimation procedure. Acoustic measures were made of: (a) signal periodicity, (b) first harmonic amplitude, and (c) spectral tilt. Periodicity measures provided the most accurate predictions of perceived breathiness, accounting for approximately 80\% of the variance in breathiness ratings. The relative amplitude of the first harmonic correlated moderately with breathiness ratings, and two measures of spectral tilt correlated weakly with perceived breathiness.}}, 
pages = {769--778}, 
number = {4}, 
volume = {37}, 
keywords = {}
}

@article{Krom.1993.10.1044/jshr.3602.254, 
year = {1993}, 
title = {{A Cepstrum-Based Technique for Determining a Harmonics-to-Noise Ratio in Speech Signals}}, 
author = {Krom, Guus de}, 
journal = {Journal of Speech, Language, and Hearing Research}, 
issn = {1092-4388}, 
doi = {10.1044/jshr.3602.254}, 
pmid = {8487518}, 
abstract = {{A new method to calculate a spectral harmonics-to-noise ratio (HNR) in speech signals is presented. The method involves discrimination between harmonic and noise energy in the magnitude spectrum by means of a comb-liftering operation in the cepstrum domain. Sensitivity of HNR to (a) additive noise and (b) jitter was tested with synthetic vowel-like signals, generated at 10 fundamental frequencies. All jitter and noise signals were analyzed at three window lengths in order to investigate the effect of the length of the analysis frame on the estimated HNR values. Results of a multiple linear regression analysis with noise or jitter, F0, and window length as predictors for HNR indicate a major effect of both noise and jitter on HNR, in that HNR decreases almost linearly with increasing noise levels or increasing jitter. The influence of F0 and window length on HNR is small for the jittered signals, but HNR increases considerably with increasing F0 or window length for the noise signals. We conclude that the method seems to be a valid technique for determining the amount of spectral noise, because it is almost linearly sensitive to both noise and jitter for a large part of the noise or jitter continuum. The strong negative relation between HNR and jitter illustrates that spectral noise measures cannot simply be taken as indicators of the actual amount of noise in the time signal. Instead, HNR integrates several aspects of the acoustic stability of the signal. As such, HNR may be a useful parameter in the analysis of voice quality, although it cannot be directly interpreted in terms of underlying glottal events or perceptual characteristics.}}, 
pages = {254--266}, 
number = {2}, 
volume = {36}, 
keywords = {}
}

@article{Latoszek.2019.10.1002/lary.27350, 
year = {2019}, 
title = {{Diagnostic Accuracy of Dysphonia Classification of DSI and AVQI}}, 
author = {Latoszek, Ben Barsties v. and Ulozaitė‐Stanienė, Nora and Petrauskas, Tadas and Uloza, Virgilijus and Maryn, Youri}, 
journal = {The Laryngoscope}, 
issn = {0023-852X}, 
doi = {10.1002/lary.27350}, 
pmid = {30203473}, 
abstract = {{The Dysphonia Severity Index (DSI) and the Acoustic Voice Quality Index (AVQI) have been successfully investigated to quantify voice quality. The aim of the present study was to evaluate the diagnostic accuracy of both measurements in comparison with the dysphonia classification. In total, 264 subjects with vocally healthy voices (n = 105) and with various voice disorders (n = 159) were included in the study. To determine the dysphonia classification, all subjects underwent a videolaryngostroboscopy and, if necessary, a direct microlaryngoscopy plus a clinical examination to diagnose a voice disorder. Patients with a vocally healthy voice had no actual voice complaints, no history of chronic laryngeal diseases or voice disorders, no hearing problems and were determined as healthy voices by clinical voice specialists. To evaluate the diagnostic accuracy, receiver operating characteristic statistics and correct classification rate (CCR) were used. The diagnostic accuracy of DSI and AVQI showed strong sensitivity and specificity in the determination of dysphonia classification. A DSI threshold of 3.05 obtained a high sensitivity of 94.3\% and specificity of 84.3\%. An CCR of 88\% was determined for DSI. Also, an AVQI threshold of 3.31 showed reasonable sensitivity of 71.7\% and specificity of 88\%. The CCR for AVQI was 79\%. Although DSI and AVQI were developed to quantify voice quality, the present results showed that both measurements can evaluate the dysphonia classification as well. Particularly, the DSI might have higher potential in the evaluation of dysphonia classification. 2C Laryngoscope, 129:692–698, 2019}}, 
pages = {692--698}, 
number = {3}, 
volume = {129}, 
keywords = {}
}

@article{Maryn.2017.10.1016/j.jvoice.2017.01.002, 
year = {2017}, 
title = {{Measuring the Dysphonia Severity Index (DSI) in the Program Praat}}, 
author = {Maryn, Youri and Morsomme, Dominique and Bodt, Marc De}, 
journal = {Journal of Voice}, 
issn = {0892-1997}, 
doi = {10.1016/j.jvoice.2017.01.002}, 
pmid = {28187924}, 
abstract = {{BackgroundThe original Dysphonia Severity Index (ie, DSI) weighs and combines four voice markers in a single number to size dysphonia gradation in the clinic: percent jitter (from Multi-Dimensional Voice Program; KayPENTAX Corp., Montvale, NJ), softest intensity and highest fundamental frequency (both from Voice Range Profile; KayPENTAX Corp.), and maximum phonation time. To be more generally applied, however, implementation of DSI in the program Praat (Paul Boersma and David Weenink, Institute for Phonetic Sciences, University of Amsterdam, The Netherlands) would be advantageous for all voice clinicians. The presented project was therefore designed (1) to develop such a Praat application and (b) to test its concurrent validity.MethodsThe four voice markers for the original DSI, as well as ten additional voice markers in Praat, were administered on a total of 49 subjects in three different clinical voice centers. A crossover research design was implemented to counterbalance for possible exercise effects. First, stepwise multiple linear regression was applied to build a statistical model with the best combination of Praat predictors for the original DSI. Second, correlation statistics were applied to compare Praat's DSI with the original DSI.ResultsBoth DSI versions correlated strongly. A custom script was therefore written for automated DSI determination in Praat.ConclusionWith this script, every voice clinician can easily determine DSI in the Praat program.}}, 
pages = {644.e29--644.e40}, 
number = {5}, 
volume = {31}, 
keywords = {}
}

@article{Wuyts:2000vb, 
year = {2000-06}, 
title = {{The Dysphonia Severity Index: an objective measure of vocal quality based on a multiparameter approach.}}, 
author = {Wuyts, F L and Bodt, M S De and Molenberghs, G and Remacle, M and Heylen, L and Millet, B and Lierde, K Van and Raes, J and Heyning, P H Van de}, 
journal = {Journal of Speech, Language and Hearing Research}, 
doi = {10.1044/jslhr.4303.796}, 
url = {https://pubs.asha.org/doi/abs/10.1044/jslhr.4303.796}, 
abstract = {{The vocal quality of a patient is modeled by means of a Dysphonia Severity Index (DSI), which is designed to establish an objective and quantitative correlate of the perceived vocal quality. The DSI is based on the weighted combination of the following selected set of voice measurements: highest frequency (F(0)-High in Hz), lowest intensity (I-Low in dB), maximum phonation time (MPT in s), and jitter (\%). The DSI is derived from a multivariate analysis of 387 subjects with the goal of describing, purely based on objective measures, the perceived voice quality. It is constructed as DSI = 0.13 x MPT + 0.0053 x F(0)-High - 0.26 x I-Low - 1.18 x Jitter (\%) + 12.4. The DSI for perceptually normal voices equals +5 and for severely dysphonic voices -5. The more negative the patient's index, the worse is his or her vocal quality. As such, the DSI is especially useful to evaluate therapeutic evolution of dysphonic patients. Additionally, there is a high correlation between the DSI and the Voice Handicap Index score.}}, 
pages = {796 -- 809}, 
number = {3}, 
volume = {43}, 
language = {English}, 
keywords = {}
}

@article{Sobol.2020.10.1016/j.jvoice.2020.04.010, 
year = {2020}, 
title = {{The Dysphonia Severity Index (DSI)—Normative Values. Systematic Review and Meta-Analysis}}, 
author = {Sobol, Maria and Sielska-Badurek, Ewelina M.}, 
journal = {Journal of Voice}, 
issn = {0892-1997}, 
doi = {10.1016/j.jvoice.2020.04.010}, 
pmid = {32381275}, 
abstract = {{Introduction As the dysphonia severity index (DSI) is used in clinical practice as a diagnostic tool, a thorough systematic review of the literature is required to assess the normative value of DSI. The main propose of present study was to determine the normative value of DSI among subjects whose voices were judged as normal. Methods A systematic literature search was performed using PubMed to access relevant databases and to locate outcome studies. Eligibility criteria included type of publication, participant characteristics and report of outcomes. Data analysis was conducted using meta-analysis method. Results Fourteen articles were included for the final analysis. The normative value of DSI equals 3.05 with confidential range 2.13–3.98 was received for group of 1330 of healthy subjects whose voices were judged as normal, with range age 17.3–94 years. Conclusions As DSI value is stable over subject groups and used as a diagnostic tool for adults a thorough systematic review of the literature is required to assess the normative value of DSI. Mean normative value of the DSI was found to be 3.05 with the DSI confidence levels between 2.13 and 3.98, which is lower than given previously.}}, 
keywords = {}
}


@inproceedings{Bruckl2017, 
year = {2017}, 
author = {Brückl, MAE and Ibragimova, Elvira and Bögelein, Silke}, 
title = {{Acoustic Tremor Measurement: Comparing Two Systems}}, 
booktitle = {Proceedings of the 10th International Workshop on Models and Analysis of Vocal Emissions for Biomedical Applications (MAVEBA)}, 
pages = {19--22}, 
keywords = {}
}

@Misc{tremor305,
  author = {Brückl, MAE},
  title = {Praat-script for the computation of 18 measures of (vocal) tremor},
  doi = {10.13140/RG.2.2.13850.57287}, 
  version = {3.05},
  year = {2021},
  date = {2021-07-01},
}

@article{Escudero.2009.10.1121/1.3180321, 
year = {2009-09}, 
rating = {0}, 
title = {{A cross-dialect acoustic description of vowels: Brazilian and European Portuguese}}, 
author = {Escudero, Paola and Boersma, Paul and Rauber, Andréia Schurt and Bion, Ricardo A H}, 
journal = {The Journal of the Acoustical Society of America}, 
issn = {0001-4966}, 
doi = {10.1121/1.3180321}, 
pmid = {19739752}, 
abstract = {{This paper examines four acoustic correlates of vowel identity in Brazilian Portuguese (BP) and European Portuguese (EP): first formant (F1), second formant (F2), duration, and fundamental frequency (F0). Both varieties of Portuguese display some cross-linguistically common phenomena: vowel-intrinsic duration, vowel-intrinsic pitch, gender-dependent size of the vowel space, gender-dependent duration, and a skewed symmetry in F1 between front and back vowels. Also, the average difference between the vocal tract sizes associated with /i/ and /u/, as measured from formant analyses, is comparable to the average difference between male and female vocal tract sizes. A language-specific phenomenon is that in both varieties of Portuguese the vowel-intrinsic duration effect is larger than in many other languages. Differences between BP and EP are found in duration (BP has longer stressed vowels than EP), in F1 (the lower-mid front vowel approaches its higher-mid counterpart more closely in EP than in BP), and in the size of the intrinsic pitch effect (larger for BP than for EP).}}, 
pages = {1379--1393}, 
number = {3}, 
volume = {126}, 
language = {English}, 
keywords = {}
}

@inproceedings{Weenink2015, 
author = {Weenink, David and others}, 
title = {{Improved formant frequency measurements of short segments.}}, 
year={2015},
booktitle = {ICPhS}
}


@inproceedings{Boersma1993, 
year = {1993}, 
author = {Boersma, Paul}, 
title = {{Accurate short-term analysis of the fundamental frequency and the harmonics-to-noise ratio of a sampled sound}}, 
booktitle = {Proceedings of the institute of phonetic sciences}, 
pages = {97--110}, 
volume = {17}
}

@article{Cohen.1995.10.1121/1.413512, 
year = {1995}, 
title = {{A spectral network model of pitch perception}}, 
author = {Cohen, Michael A and Grossberg, Stephen and Wyse, Lonce L}, 
journal = {The Journal of the Acoustical Society of America}, 
issn = {0001-4966}, 
doi = {10.1121/1.413512}, 
pmid = {7642825}, 
abstract = {{A model of pitch perception, called the spatial pitch network or SPINET model, is developed and analyzed. The model neurally instantiates ideas from the spectral pitch modeling literature and joins them to basic neural network signal processing designs to stimulate a broader range of perceptual pitch data than previous spectral models. The components of the model are interpreted as peripheral mechanical and neural processing stages, which are capable of being incorporated into a larger network architecture for separating multiple sound sources in the environment. The core of the new model transforms a spectral representation of an acoustic source into a spatial distribution of pitch strengths. The SPINET model uses a weighted "harmonic sieve" whereby the strength of activation of a given pitch depends upon a weighted sum of narrow regions around the harmonics of the nominal pitch value, and higher harmonics contribute less to a pitch than lower ones. Suitably chosen harmonic weighting functions enable computer simulations of pitch perception data involving mistuned components, shifted harmonics, and various types of continuous spectra including rippled noise. It is shown how the weighting functions produce the dominance region, how they lead to octave shifts of pitch in response to ambiguous stimuli, and how they lead to a pitch region in response to the octave-spaced Shepard tone complexes and Deutsch tritones without the use of attentional mechanisms to limit pitch choices. An on-center off-surround network in the model helps to produce noise suppression, partial masking, and edge pitch. Finally, it is shown how peripheral filtering and short-term energy measurements produce a model pitch estimate that is sensitive to certain component phase relationships.}}, 
pages = {862--879}, 
number = {2}, 
volume = {98}
}


@article{Hermes.10.1121/1.396427, 
year = {1988}, 
title = {{Measurement of pitch by subharmonic summation}}, 
author = {Hermes, Dik J}, 
journal = {The Journal of the Acoustical Society of America}, 
issn = {0001-4966}, 
doi = {10.1121/1.396427}, 
pmid = {3343445}, 
pages = {257--264}, 
number = {1}, 
volume = {83}, 
}

@article{Camacho.2008.10.1121/1.2951592, 
year = {2008}, 
title = {{A sawtooth waveform inspired pitch estimator for speech and music}}, 
author = {Camacho, Arturo and Harris, John G.}, 
journal = {The Journal of the Acoustical Society of America}, 
issn = {0001-4966}, 
doi = {10.1121/1.2951592}, 
pmid = {19045655}, 
abstract = {{A sawtooth waveform inspired pitch estimator (SWIPE) has been developed for speech and music. SWIPE estimates the pitch as the fundamental frequency of the sawtooth waveform whose spectrum best matches the spectrum of the input signal. The comparison of the spectra is done by computing a normalized inner product between the spectrum of the signal and a modified cosine. The size of the analysis window is chosen appropriately to make the width of the main lobes of the spectrum match the width of the positive lobes of the cosine. SWIPE‚Ä≤, a variation of SWIPE, utilizes only the first and prime harmonics of the signal, which significantly reduces subharmonic errors commonly found in other pitch estimation algorithms. The authors‚Äô tests indicate that SWIPE and SWIPE‚Ä≤ performed better on two spoken speech and one disordered voice database and one musical instrument database consisting of single notes performed at a variety of pitches.}}, 
pages = {1638--1652}, 
number = {3}, 
volume = {124}
}

@misc{sptkspeech,
  title={Speech Signal Processing Toolkit (SPTK), 2017},
  author={SPTK Working Group and others},
  url={http://sp-tk.sourceforge. net},
  year=2017
}

@article{talkin1995robust,
  title={A robust algorithm for pitch tracking (RAPT)},
  author={Talkin, David and Kleijn, W Bastiaan},
  journal={Speech coding and synthesis},
  volume={495},
  pages={518},
  year={1995}
}


@misc{talkin2019reaper,
  title={REAPER: Robust epoch and pitch estimator},
  author={Talkin, David},
  url={https://github.com/google/REAPER},
  year={2019}
}


@article{Jiao.2012.10.1109/cicn.2012.163, 
year = {2012}, 
title = {{Pitch Detection Algorithm Based on NCCF and CAMDF}}, 
author = {Jiao, He and Zhimi, He and Chaocheng, Xie}, 
journal = {2012 Fourth International Conference on Computational Intelligence and Communication Networks}, 
doi = {10.1109/cicn.2012.163}, 
abstract = {{This paper describes the pitch detection techniques and voiced/unvoiced decision using normalized cross correlation function (NCCF) method and circular average magnitude difference function (CAMDF) method involving the preprocessing and the post processing. Major innovations include: The pitch period candidates selection based on NCCF and CAMDF; the simple and easy to implement post processing based on local speech segment. It also presents the basic experiments and discussions.}}, 
pages = {661--664}, 
volume = {1}, 
keywords = {}
}

@article{yang2021torchaudio,
  title={TorchAudio: Building Blocks for Audio and Speech Processing},
  author={Yao-Yuan Yang and Moto Hira and Zhaoheng Ni and Anjali Chourdia and Artyom Astafurov and Caroline Chen and Ching-Feng Yeh and Christian Puhrsch and David Pollack and Dmitriy Genzel and Donny Greenberg and Edward Z. Yang and Jason Lian and Jay Mahadeokar and Jeff Hwang and Ji Chen and Peter Goldsborough and Prabhat Roy and Sean Narenthiran and Shinji Watanabe and Soumith Chintala and Vincent Quenneville-Bélair and Yangyang Shi},
  journal={arXiv preprint arXiv:2110.15018},
  year={2021}
}


@article{Kasi.2002.10.1109/icassp.2002.5743729, 
year = {2002}, 
title = {{Yet Another Algorithm for Pitch Tracking}}, 
author = {Kasi, Kavita and Zahorian, Stephen A.}, 
journal = {2002 IEEE International Conference on Acoustics, Speech, and Signal Processing}, 
doi = {10.1109/icassp.2002.5743729}, 
abstract = {{In this paper, we present a pitch detection algorithm that is extremely robust for both high quality and telephone speech. The kernel method for this algorithm is the “NCCF or Normalized Cross Correlation” reported by David Talkin [1]. Major innovations include: processing of the original acoustic signal and a nonlinearly processed version of the signal to partially restore very weak F0 components; intelligent peak picking to select multiple F0 candidates and assign merit factors; and, incorporation of highly robust pitch contours obtained from smoothed versions of low frequency portions of spectrograms. Dynamic programming is used to find the “best” pitch track among all the candidates, using both local and transition costs. We evaluated our algorithm using the Keele pitch extraction reference database as “ground truth” for both “high quality” and “telephone” speech. For both types of speech, the error rates obtained are lower than the lowest reported in the literature.}}, 
pages = {I--361-I-364}, 
volume = {1}
}


@article{Kim.2018.10.1109/icassp.2018.8461329, 
year = {2018}, 
title = {{Crepe: A Convolutional Representation for Pitch Estimation}}, 
author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo}, 
journal = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
doi = {10.1109/icassp.2018.8461329}, 
abstract = {{The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.}}, 
pages = {161--165}, 
volume = {00}
}



@article{Cheveigné.2002.10.1121/1.1458024, 
year = {2002}, 
title = {{YIN, a fundamental frequency estimator for speech and music}}, 
author = {Cheveigné, Alain de and Kawahara, Hideki}, 
journal = {The Journal of the Acoustical Society of America}, 
issn = {0001-4966}, 
doi = {10.1121/1.1458024}, 
pmid = {12002874}, 
abstract = {{An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.}}, 
pages = {1917--1930}, 
number = {4}, 
volume = {111}
}


@article{Mauch.2014.10.1109/icassp.2014.6853678, 
year = {2014}, 
title = {{PYIN: A Fundamental Frequency Estimator using Probabilistic Threshold Distributions}}, 
author = {Mauch, Matthias and Dixon, Simon}, 
journal = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
doi = {10.1109/icassp.2014.6853678}, 
abstract = {{We propose the Probabilistic YIN (PYIN) algorithm, a modification of the well-known YIN algorithm for fundamental frequency (F0) estimation. Conventional YIN is a simple yet effective method for frame-wise monophonic F0 estimation and remains one of the most popular methods in this domain. In order to eliminate short-term errors, outputs of frequency estimators are usually post-processed resulting in a smoother pitch track. One shortcoming of YIN is that such post-processing cannot fall back on alternative interpretations of the signal because the method outputs precisely one estimate per frame. To address this problem we modify YIN to output multiple pitch candidates with associated probabilities (PYIN Stage 1). These probabilities arise naturally from a prior distribution on the YIN threshold parameter. We use these probabilities as observations in a hidden Markov model, which is Viterbi-decoded to produce an improved pitch track (PYIN Stage 2). We demonstrate that the combination of Stages 1 and 2 raises recall and precision substantially. The additional computational complexity of PYIN over YIN is low. We make the method freely available online11 http://code.soundsoftware.ac.uk/projects/pyin as an open source C++ library for Vamp hosts. http://code.soundsoftware.ac.uk/projects/pyin}}, 
pages = {659--663}
}

@misc{brian_mcfee_2022_6097378,
  author       = {Brian McFee and
                  Alexandros Metsai and
                  Matt McVicar and
                  Stefan Balke and
                  Carl Thomé and
                  Colin Raffel and
                  Frank Zalkow and
                  Ayoub Malek and
                  Dana and
                  Kyungyun Lee and
                  Oriol Nieto and
                  Dan Ellis and
                  Jack Mason and
                  Eric Battenberg and
                  Scott Seyfarth and
                  Ryuichi Yamamoto and
                  viktorandreevichmorozov and
                  Keunwoo Choi and
                  Josh Moore and
                  Rachel Bittner and
                  Shunsuke Hidaka and
                  Ziyao Wei and
                  nullmightybofo and
                  Adam Weiss and
                  Darío Hereñú and
                  Fabian-Robert Stöter and
                  Pius Friesch and
                  Matt Vollrath and
                  Taewoon Kim and
                  Thassilo},
  title        = {librosa/librosa: 0.9.1},
  month        = feb,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {0.9.1},
  doi          = {10.5281/zenodo.6097378},
  url          = {https://doi.org/10.5281/zenodo.6097378}
}


@article{Ghahremani.2014.10.1109/icassp.2014.6854049, 
year = {2014}, 
title = {{A Pitch Extraction Algorithm Tuned for Automatic Speech Recognition}}, 
author = {Ghahremani, Pegah and BabaAli, Bagher and Povey, Daniel and Riedhammer, Korbinian and Trmal, Jan and Khudanpur, Sanjeev}, 
journal = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
doi = {10.1109/icassp.2014.6854049}, 
abstract = {{In this paper we present an algorithm that produces pitch and probability-of-voicing estimates for use as features in automatic speech recognition systems. These features give large performance improvements on tonal languages for ASR systems, and even substantial improvements for non-tonal languages. Our method, which we are calling the Kaldi pitch tracker (because we are adding it to the Kaldi ASR toolkit), is a highly modified version of the getf0 (RAPT) algorithm. Unlike the original getf0 we do not make a hard decision whether any given frame is voiced or unvoiced; instead, we assign a pitch even to unvoiced frames while constraining the pitch trajectory to be continuous. Our algorithm also produces a quantity that can be used as a probability of voicing measure; it is based on the normalized autocorrelation measure that our pitch extractor uses. We present results on data from various languages in the BABEL project, and show a large improvement over systems without tonal features and systems where pitch and POV information was obtained from SAcC or getf0.}}, 
pages = {2494--2498}, 
keywords = {}
}


@article{Morise.2017.10.21437/interspeech.2017-68, 
year = {2017}, 
title = {{Harvest: A High-Performance Fundamental Frequency Estimator from Speech Signals}}, 
author = {Morise, Masanori}, 
journal = {Interspeech 2017}, 
doi = {10.21437/interspeech.2017-68}, 
pages = {2321--2325}, 
keywords = {}
}


@article{MORISE.2016.10.1587/transinf.2015edp7457, 
year = {2016}, 
title = {{WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications}}, 
author = {MORISE, Masanori and YOKOMORI, Fumiya and OZAWA, Kenji}, 
journal = {IEICE Transactions on Information and Systems}, 
issn = {0916-8532}, 
doi = {10.1587/transinf.2015edp7457}, 
pages = {1877--1884}, 
number = {7}, 
volume = {E99.D}, 
keywords = {}



@article{morise2010rapid, 
year = {2010}, 
title = {{Rapid F0 estimation for high-SNR speech based on fundamental component extraction}}, 
author = {Morise, Masanori and Kawahara, Hideki and Nishiura, Takanobu}, 
journal = {Trans. IEICEJ}, 
pages = {109--117}, 
volume = {93}
}


@phdthesis{Tsanas:2012un, 
year = {2012}, 
title = {{Accurate telemonitoring of Parkinson’s disease symptom severity using nonlinear speech signal processing and statistical machine learning}}, 
author = {Tsanas, Athanasios}, 
abstract = {{<p>This study focuses on the development of an objective, automated method to extract clinically useful information from sustained vowel phonations in the context of Parkinson’s disease (PD). The aim is twofold: (a) differentiate PD subjects from healthy controls, and (b) replicate the Unified Parkinson’s Disease Rating Scale (UPDRS) metric which provides a <em>clinical impression</em> of PD symptom severity. This metric spans the range 0 to 176, where 0 denotes a healthy person and 176 total disability. Currently, UPDRS assessment requires the physical presence of the subject in the clinic, is <em>subjective</em> relying on the clinical rater’s expertise, and logistically costly for national health systems. Hence, the practical frequency of symptom tracking is typically confined to once every several months, hindering recruitment for large-scale clinical trials and under-representing the true time scale of PD fluctuations.</p><p>We develop a comprehensive framework to analyze speech signals by: (1) extracting novel, distinctive signal <em>features</em>, (2) using robust <em>feature selection</em> techniques to obtain a parsimonious subset of those features, and (3a) differentiating PD subjects from healthy controls, or (3b) determining UPDRS using powerful <em>statistical machine learning</em> tools. Towards this aim, we also investigate 10 existing fundamental frequency (F\_0) estimation algorithms to determine the most useful algorithm for this application, and propose a novel ensemble F\_0 estimation algorithm which leads to a 10\% improvement in accuracy over the best individual approach. Moreover, we propose novel feature selection schemes which are shown to be very competitive against widely-used schemes which are more complex. We demonstrate that we can successfully differentiate PD subjects from healthy controls with 98.5\% overall accuracy, and also provide <em>rapid</em>, <em>objective</em>, and <em>remote</em> replication of UPDRS assessment with clinically useful accuracy (approximately 2 UPDRS points from the clinicians’ estimates), using only simple, self-administered, and non-invasive speech tests.</p><p>The findings of this study strongly support the use of speech signal analysis as an objective basis for practical clinical decision support tools in the context of PD assessment.</p>}}, 
school = {Oxford University, UK}
}



@article{Tsanas:2011cb, 
year = {2011}, 
title = {{Nonlinear speech analysis algorithms mapped to a standard metric achieve clinically useful quantification of average Parkinson's disease symptom severity.}}, 
author = {Tsanas, Athanasios and Little, Max A and McSharry, Patrick E and Ramig, Lorraine Olson}, 
journal = {Journal of the Royal Society, Interface / the Royal Society}, 
doi = {10.1098/rsif.2010.0456}, 
abstract = {{The standard reference clinical score quantifying average Parkinson's disease (PD) symptom severity is the Unified Parkinson's Disease Rating Scale (UPDRS). At present, UPDRS is determined by the subjective clinical evaluation of the patient's ability to adequately cope with a range of tasks. In this study, we extend recent findings that UPDRS can be objectively assessed to clinically useful accuracy using simple, self-administered speech tests, without requiring the patient's physical presence in the clinic. We apply a wide range of known speech signal processing algorithms to a large database (approx. 6000 recordings from 42 PD patients, recruited to a six-month, multi-centre trial) and propose a number of novel, nonlinear signal processing algorithms which reveal pathological characteristics in PD more accurately than existing approaches. Robust feature selection algorithms select the optimal subset of these algorithms, which is fed into non-parametric regression and classification algorithms, mapping the signal processing algorithm outputs to UPDRS. We demonstrate rapid, accurate replication of the UPDRS assessment with clinically useful accuracy (about 2 UPDRS points difference from the clinicians' estimates, p<0.001). This study supports the viability of frequent, remote, cost-effective, objective, accurate UPDRS telemonitoring based on self-administered speech tests. This technology could facilitate large-scale clinical trials into novel PD treatments.}}, 
pages = {842 -- 855}, 
number = {59}, 
volume = {8}, 
language = {English}, 
keywords = {}, 
month = {06}
}

@inproceedings{tsanas2013automatic,
  title={Automatic objective biomarkers of neurodegenerative disorders using nonlinear speech signal processing tools},
  author={Tsanas, A},
  booktitle={8th International Workshop on Models and Analysis of Vocal Emissions for Biomedical Applications (MAVEBA)},
  pages={37--40},
  year={2013}
}

@article{brookes2011voicebox,
  title={Voicebox: speech processing toolbox for MATLAB [software]},
  author={Brookes, M},
  journal={Imperial College, London},
  year={2011}
}

@article{little2007exploiting,
  title={Exploiting nonlinear recurrence and fractal scaling properties for voice disorder detection},
  author={Little, Max and Mcsharry, Patrick and Roberts, Stephen and Costello, Declan and Moroz, Irene},
  journal={Nature Precedings},
  pages={1--1},
  year={2007},
  publisher={Nature Publishing Group}
}

@inproceedings{little2006nonlinear,
  title={Nonlinear, biophysically-informed speech pathology detection},
  author={Little, Max and McSharry, Patrick and Moroz, Irene and Roberts, Stephen},
  booktitle={2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings},
  volume={2},
  pages={II--II},
  year={2006},
  organization={IEEE}
}


@article{Morise.2016.10.1016/j.specom.2016.09.001, 
year = {2016}, 
title = {{D4C, a band-aperiodicity estimator for high-quality speech synthesis}}, 
author = {Morise, Masanori}, 
journal = {Speech Communication}, 
issn = {0167-6393}, 
doi = {10.1016/j.specom.2016.09.001}, 
abstract = {{An algorithm is proposed for estimating the band aperiodicity of speech signals, where “aperiodicity” is defined as the power ratio between the speech signal and the aperiodic component of the signal. Since this power ratio depends on the frequency band, the aperiodicity should be given for several frequency bands. The proposed D4C (Definitive Decomposition Derived Dirt-Cheap) estimator is based on an extension of a temporally static group delay representation of periodic signals. In this paper, the principle and algorithm of D4C are explained, and its effectiveness is discussed with reference to objective and subjective evaluations. Evaluation results indicate that a speech synthesis system using D4C can synthesize natural speech better than ones using other algorithms.}}, 
pages = {57--65}, 
volume = {84}
}


@article{Morise:2015ia, 
year = {2015-03}, 
rating = {0}, 
title = {{CheapTrick, a spectral envelope estimator for high-quality speech synthesis}}, 
author = {Morise, Masanori}, 
journal = {Speech Communication}, 
doi = {10.1016/j.specom.2014.09.003}, 
url = {http://www.sciencedirect.com/science/article/pii/S0167639314000697}, 
abstract = {{Abstract A spectral envelope estimation algorithm is presented to achieve high-quality speech synthesis. The concept of the algorithm is to obtain an accurate and temporally stable spectral envelope. The algorithm uses fundamental frequency (F0) and consists of F0-adaptive windowing, smoothing of the power spectrum, and spectral recovery in the quefrency domain. Objective and subjective evaluations were carried out to demonstrate the effectiveness of the proposed algorithm. Results of both evaluations indicated that the proposed algorithm can obtain a temporally stable spectral envelope and synthesize speech with higher sound quality than speech synthesized with other algorithms.}}, 
pages = {1 -- 7}, 
number = {0}, 
volume = {67}, 
language = {English}
}