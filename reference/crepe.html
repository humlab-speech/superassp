<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="The CREPE (Kim et al. 2018)
 applies a deep convolutional neural network directly on the time-domain waveform to find
the fundamental frequency in a speech signal. Two versions of the models have been trained, one smaller yielding quicker results, and the full model which can be considerably
more computationally intensive to apply."><title>Compute pitch and periodicity using the CREPE pitch tracker — crepe • superassp</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Compute pitch and periodicity using the CREPE pitch tracker — crepe"><meta property="og:description" content="The CREPE (Kim et al. 2018)
 applies a deep convolutional neural network directly on the time-domain waveform to find
the fundamental frequency in a speech signal. Two versions of the models have been trained, one smaller yielding quicker results, and the full model which can be considerably
more computationally intensive to apply."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">superassp</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.1</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"></ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Compute pitch and periodicity using the CREPE pitch tracker</h1>
      
      <div class="d-none name"><code>crepe.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>The CREPE (Kim et al. 2018)
 applies a deep convolutional neural network directly on the time-domain waveform to find
the fundamental frequency in a speech signal. Two versions of the models have been trained, one smaller yielding quicker results, and the full model which can be considerably
more computationally intensive to apply.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">crepe</span><span class="op">(</span></span>
<span>  <span class="va">listOfFiles</span>,</span>
<span>  beginTime <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  endTime <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  windowShift <span class="op">=</span> <span class="fl">5</span>,</span>
<span>  windowSize <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  minF <span class="op">=</span> <span class="fl">70</span>,</span>
<span>  maxF <span class="op">=</span> <span class="fl">200</span>,</span>
<span>  voicing.threshold <span class="op">=</span> <span class="fl">0.21</span>,</span>
<span>  silence.threshold <span class="op">=</span> <span class="op">-</span><span class="fl">60</span>,</span>
<span>  model <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"tiny"</span>, <span class="st">"full"</span><span class="op">)</span>,</span>
<span>  explicitExt <span class="op">=</span> <span class="st">"crp"</span>,</span>
<span>  outputDirectory <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  toFile <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  conda.env <span class="op">=</span> <span class="cn">NULL</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>listOfFiles</dt>
<dd><p>A vector of file paths to wav files.</p></dd>


<dt>beginTime</dt>
<dd><p>(Not implemented) The start time of the section of the sound file that should be processed.</p></dd>


<dt>endTime</dt>
<dd><p>(Not implemented) The end time of the section of the sound file that should be processed.</p></dd>


<dt>windowShift</dt>
<dd><p>The measurement interval (frame duration), in seconds.</p></dd>


<dt>windowSize</dt>
<dd><p>the analysis window length (in ms).</p></dd>


<dt>minF</dt>
<dd><p>Candidate f0 frequencies below this frequency will not be considered.</p></dd>


<dt>maxF</dt>
<dd><p>Candidates above this frequency will be ignored.</p></dd>


<dt>voicing.threshold</dt>
<dd><p>Voice/unvoiced threshold. Default is 0.21.</p></dd>


<dt>silence.threshold</dt>
<dd><p>Frames that do not contain amplitudes above this threshold (relative to the global maximum amplitude), are probably silent.</p></dd>


<dt>model</dt>
<dd><p>Use a fast ("tiny") model, or a more complete ("full") model to find pitch. The more complete model will take approximately 9-11 times longer to process the file.</p></dd>


<dt>explicitExt</dt>
<dd><p>the file extension that should be used.</p></dd>


<dt>outputDirectory</dt>
<dd><p>set an explicit directory for where the signal file will be written. If not defined, the file will be written to the same directory as the sound file.</p></dd>


<dt>toFile</dt>
<dd><p>write the output to a file? The file will be written in  <code>outputDirectory</code>, if defined, or in the same directory as the soundfile.</p></dd>


<dt>conda.env</dt>
<dd><p>The name of the conda environment in which Python and its
required packages are stored. Please make sure that you know what you are
doing if you change this. Defaults to <code>NULL</code>, which means that the default enviroment or the environment set in the
<code>RETICULATE_PYTHON</code> environment variable will be used.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    

<p>An SSFF track object containing two tracks (f0 and periodicity) that are either returned (toFile == FALSE) or stored on disk.</p>
    </div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>T</p>
    </div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Kim JW, Salamon J, Li P, Bello JP (2018).
“Crepe: A Convolutional Representation for Pitch Estimation.”
<em>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, <b>00</b>, 161--165.
<a href="https://doi.org/10.1109/icassp.2018.8461329" class="external-link">doi:10.1109/icassp.2018.8461329</a>
.</p>
    </div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by Fredrik Karlsson.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer></div>

  

  

  </body></html>

