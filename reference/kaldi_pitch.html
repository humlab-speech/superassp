<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="The algorithm used is a version of the RAPT algorithm
that considers voicing also in voiceless frames and conputes a
Normalized Cross Correlation Function (NCCF) that can be used to
estimate the probability of voicing (Ghahremani et al. 2014)
."><title>Estimate pitch using the Kaldi modifies version of RAPT — kaldi_pitch • superassp</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Estimate pitch using the Kaldi modifies version of RAPT — kaldi_pitch"><meta property="og:description" content="The algorithm used is a version of the RAPT algorithm
that considers voicing also in voiceless frames and conputes a
Normalized Cross Correlation Function (NCCF) that can be used to
estimate the probability of voicing (Ghahremani et al. 2014)
."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">superassp</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.1</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"></ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Estimate pitch using the Kaldi modifies version of RAPT</h1>
      
      <div class="d-none name"><code>kaldi_pitch.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>The algorithm used is a version of the <a href="rapt.html">RAPT</a> algorithm
that considers voicing also in voiceless frames and conputes a
Normalized Cross Correlation Function (NCCF) that can be used to
estimate the probability of voicing (Ghahremani et al. 2014)
.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">kaldi_pitch</span><span class="op">(</span></span>
<span>  <span class="va">listOfFiles</span>,</span>
<span>  beginTime <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  endTime <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  windowShift <span class="op">=</span> <span class="fl">5</span>,</span>
<span>  windowSize <span class="op">=</span> <span class="fl">25</span>,</span>
<span>  minF <span class="op">=</span> <span class="fl">70</span>,</span>
<span>  maxF <span class="op">=</span> <span class="fl">200</span>,</span>
<span>  softMinF0 <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  voiced_voiceless_cost <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  owpass_cutoff <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>  resample_frequency <span class="op">=</span> <span class="fl">4000</span>,</span>
<span>  deltaChange <span class="op">=</span> <span class="fl">0.005</span>,</span>
<span>  nccfBallast <span class="op">=</span> <span class="fl">7000</span>,</span>
<span>  lowpass_cutoff <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>  lowpass_filter_width <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  upsample_filter_width <span class="op">=</span> <span class="fl">5</span>,</span>
<span>  max_frames_latency <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  frames_per_chunk <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  simulate_first_pass_online <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  recompute_frame <span class="op">=</span> <span class="fl">500</span>,</span>
<span>  snip_edges <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  explicitExt <span class="op">=</span> <span class="st">"kap"</span>,</span>
<span>  outputDirectory <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  toFile <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  conda.env <span class="op">=</span> <span class="cn">NULL</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>listOfFiles</dt>
<dd><p>A vector of file paths to wav files.</p></dd>


<dt>beginTime</dt>
<dd><p>The start time of the section of the sound file that should be processed.</p></dd>


<dt>endTime</dt>
<dd><p>The end time of the section of the sound file that should be processed.</p></dd>


<dt>windowShift</dt>
<dd><p>The measurement interval (frame duration), in seconds.</p></dd>


<dt>minF</dt>
<dd><p>Candidate f0 frequencies below this frequency will not be considered.</p></dd>


<dt>maxF</dt>
<dd><p>Candidates above this frequency will be ignored.</p></dd>


<dt>resample_frequency</dt>
<dd><p>Frequency that we down-sample the signal to. Must be more than twice <code>lowpass_cutoff</code>. (default: 4000)</p></dd>


<dt>lowpass_cutoff</dt>
<dd><p>Cutoff frequency for LowPass filter (Hz) (default: 1000)</p></dd>


<dt>lowpass_filter_width</dt>
<dd><p>Integer that determines filter width of lowpass filter, more gives sharper filter. (default: 1)</p></dd>


<dt>max_frames_latency</dt>
<dd><p>Maximum number of frames of latency that we allow pitch tracking to introduce into the feature processing (affects output only if <code>frames_per_chunk</code> &gt; 0 and <code>simulate_first_pass_online</code>=<code>TRUE</code>) (default: 0)</p></dd>


<dt>frames_per_chunk</dt>
<dd><p>The number of frames used for energy normalization. (default: 0)</p></dd>


<dt>simulate_first_pass_online</dt>
<dd><p>If true, the function will output features that correspond to what an online decoder would see in the first pass of decoding – not the final version of the features, which is the default. (default: <code>FALSE</code>) Relevant if <code>frames_per_chunk &gt; 0</code>.</p></dd>


<dt>recompute_frame</dt>
<dd><p>Only relevant for compatibility with online pitch extraction. A non-critical parameter; the frame at which we recompute some of the forward pointers, after revising our estimate of the signal energy. Relevant if <code>frames_per_chunk &gt; 0</code>. (default: 500)</p></dd>


<dt>snip_edges</dt>
<dd><p>If this is set to false, the incomplete frames near the ending edge won’t be snipped, so that the number of frames is the file size divided by the <code>windowShift</code>. This makes different types of features give the same number of frames. (default: True)</p></dd>


<dt>explicitExt</dt>
<dd><p>the file extension that should be used.</p></dd>


<dt>outputDirectory</dt>
<dd><p>set an explicit directory for where the signal file will be written. If not defined, the file will be written to the same directory as the sound file.</p></dd>


<dt>toFile</dt>
<dd><p>write the output to a file? The file will be written in  <code>outputDirectory</code>, if defined, or in the same directory as the soundfile.</p></dd>


<dt>conda.env</dt>
<dd><p>The name of the conda environment in which Python and its required packages are stored. Please make sure that you know what you are doing if you change this.</p></dd>


<dt>soft_min_f0</dt>
<dd><p>(float, optional) – Minimum f0, applied in soft way, must not exceed min-f0 (default: 10.0)</p></dd>


<dt>penalty_factor</dt>
<dd><p>Cost factor for fO change. (default: 0.1)</p></dd>


<dt>delta_pitch</dt>
<dd><p>Smallest relative change in pitch that our algorithm measures. (default: 0.005)</p></dd>


<dt>nccf_ballast</dt>
<dd><p>Increasing this factor reduces NCCF for quiet frames (default: 7000)</p></dd>


<dt>psample_filter_width</dt>
<dd><p>Integer that determines filter width when upsampling NCCF. (default: 5)</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    

<p>An SSFF track object containing two tracks (f0 and nccf) that are
either returned (toFile == FALSE) or stored on disk.</p>
    </div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>The function calls the <a href="https://github.com/pytorch/audio" class="external-link">torchaudio</a> (Yang et al. 2021)
 library to do the pitch estimates and therefore
relies on it being present in a properly set up python environment to work. Please refer to the <a href="https://pytorch.org/audio/main/generated/torchaudio.functional.compute_kaldi_pitch.html" class="external-link">torchaudio manual</a> for further information.</p>
    </div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Ghahremani P, BabaAli B, Povey D, Riedhammer K, Trmal J, Khudanpur S (2014).
“A Pitch Extraction Algorithm Tuned for Automatic Speech Recognition.”
<em>2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2494--2498.
<a href="https://doi.org/10.1109/icassp.2014.6854049" class="external-link">doi:10.1109/icassp.2014.6854049</a>
.<br><br> Yang Y, Hira M, Ni Z, Chourdia A, Astafurov A, Chen C, Yeh C, Puhrsch C, Pollack D, Genzel D, Greenberg D, Yang EZ, Lian J, Mahadeokar J, Hwang J, Chen J, Goldsborough P, Roy P, Narenthiran S, Watanabe S, Chintala S, Quenneville-Bélair V, Shi Y (2021).
“TorchAudio: Building Blocks for Audio and Speech Processing.”
<em>arXiv preprint arXiv:2110.15018</em>.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p>rapt</p></div>
    </div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by Fredrik Karlsson.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer></div>

  

  

  </body></html>

